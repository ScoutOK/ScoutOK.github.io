module.exports = [
  {
    id: 0,
    name:'Introduction to OpenCV',
    videoUrl:'https://www.youtube.com/embed/oI1GgyRU5a4',
    description:'In this video I talk about computer vision with a specific focus on the OpenCV library. I explain the basics of OpenCV such as the basic data structure and how the library handles object detection. I finish up the presentation by displaying some brief live examples of some basic functions of OpenCV, including face detection, through a web cam.',
    transcript:'Hi, my name is Scout O\'Keefe and today I\'m going to give you a brief introduction into OpenCV. so first off, we need an introduction into what computer vision is. Oh, sorry. OpenCV is an open source computer vision library. So, computer vision it\'s basically any form of computer interaction with a visual input. So, you\'ve all worked with computers for at least several weeks now and you know that computers tend to like data in numbers and images that can be converted to numbers. But our bodies, our eyes, don\'t perceive them as numbers so the computer needs a method to do that. The human visual system can actually do this quite well in most cases and you actually find that humans are very good at pattern recognition. Like babies, early on, can recognize faces can understand the difference. Which I was really fascinated with my niece between the cat between a cat and a dog because, in description, they\'re not that different, but they can understand the difference. It\'s really cool and I find this field particularly interesting because it can give us insight possibly into the complexity of our own brain and visual systems which are which is really cool.\nNow when some people think of computer vision they might picture something like the image on the right which is Terminator vision and while, you know, humanoid robots will probably need computer vision, there are a lot of other applications as well. So here\'s just a few, um. Face recognition and expression recognition like the picture on the top left. Red light traffic cameras like the picture on the top right. Gesture recognition like the bottom right. Optical text recognition like the one in the middle of the bottom which also has a lot of usage is in accessibility ,making documents accessible for people who are low vision are blind. The bottom left, manufacturing just making sure things are lined up computer vision is involved in that. And my favorite example that I found is the middle of the top which is actually tracking the ball during sports and coming up with visualizations out. Because a is a company called Hawkeye does this and actually I was really excited because I\'d seen this watching tennis and I was like “oh my gosh that\'s so cool”. So, they track the ball so then you can show exactly where it landed and be like oh that was a good call by the ref for that oh man mistake.\nSo now we\'ll get into OpenCV, particularly, so as I said before it\'s an open source computer vision and machine learning software library. It\'s bsd open-source license and it\'s free for academic and commercial use. It began as a research project at Intel in 1998 and I think it became public around 2001. It has native interfaces for C++, C, Python, Java, and MATLAB as well as bindings for node. Those are all, however, those are maintained separately so, like, the current version of OpenCV is OpenCV 3 since i was using node i only had access to OpenCV 2.4.7, I believe was what I was using. It has broad OS support, Windows, Mac, Linux, iOS, and Android so you can use on your mobile devices too. It\'s pretty cool. It also contains more than 2,500 optimized algorithms. So as you might guess with a library this large, we\'re not going to get into all of it today just some key points.\nSo we\'re just going to go over the basic data structures of OpenCV, a bit of what it can do, and then some demos that I\'ve built. So, structures. These are the main data structures that are used in OpenCV. A point, it\'s a 2d point, um, has an x and y coordinate so it\'d be like a point on the image. A size is a 2d size structure so it has a width and a height and no location. That\'s what\'s unique about it. And a rectangle is like a size with a location so it\'s fixed on in space. And you actually define a rectangle by defining its lower left-hand corner in this upper right-hand corner. A rotated rectangle is a rectangle that\'s been rotated, I think that\'s somewhat self-explanatory. And, finally, matrix. The matrix is the big one because this is how images are stored and manipulated. So matrices, the primary data structure of OpenCV. I won\'t go into too much depth on this, but it is good to brush up on matrices before using the OpenCV library especially if you\'re going to be using it in one of its more native languages. You’ll note I didn\'t end up using that many that much matrix math on my own and not much linear algebra, but remember matrices means linear algebra so if you don\'t know it I guess brush up on linear algebra before engaging with this library, um. But matrices make a certain amount of sense with images because if you have an image file you have rows and columns of pixels and matrices have rows and columns. So it\'s kind of like you have points, defining in your matrix… matrix defining all of the individual pixels. So the items on your matrix; you have rows which is the height, columns which is the width, channels so if you have a grayscale image you have one channel because it\'s defining the value of white to black; RGB you have three channels red, green, and blue  - And an interesting thing about how OpenCV works is it always wants its colors blue green red, so opposite of what I\'m used to. That was a bit surprising and frustrating at the beginning - and depth and this is you know more hard computer science stuff, how the data is stored.\n OpenCV has a lot of built-in methods to perform basic linear algebraic operations like, excuse me, dot products, cross products, and finding the inverse of a matrix which from my brief learning of linear algebra I remember being a pain trying to find the inverse of larger matrices. So one cool thing that OpenCV can help you do is object detection. So OpenCV uses Viola Jones Haar Cascade for object detection. Viola Jones was developed primarily for the purpose of face detection so there are a lot more cascades for face detection than really anything else. So what a cascade is is it\'s basically a really large HTML that tells you what to look for in an image. The built-in ones for node OpenCV had a bunch of face ones they\'d like for front facing face ones, side face ones, facial features, full human body and then some cars. They had cars and license plates because it\'s, as I said before used, in red light cameras, it\'s good to be able to identify license plates. But you can make cascades for whatever you want. A tutorial i found which seemed like it would take a lot of time because you have to have examples of it - like 40 examples and like 500 negative examples or something - but there was a tutorial for how to make a Haar cascade for a banana which seemed like it might be fun. But so it\'s important in a lot of computer vision tasks like face detection using a controller like if the not the infrared controllers because I think like we uses infrared I think but some controllers is about the camera seeing an object and I think the PlayStation Move does that, although I\'ve only, like, played with it once at the store. So and also gesture recognitions. So it\'s a pattern to check for.\nSo this is just a brief clip of the video at the URL below and this is a demonstration of the Haar cascade. And so you can see it\'s going through and checking the patterns. I mean this doesn\'t really tell you what the patterns it\'s checking for are, but you can see if you can see the faint red line that red box leaves behind that means it\'s found a face. And obviously this is a lot slower than the actual processing speed. The good thing about the cascades is they\'re actually pretty quick compared to other ways to do things and so that\'s why you can, you know, in a camera sometimes have pictures, you know, identify faces in the frame before the picture is even taken. And this is just a small section of a four-minute video if you want to see more of it. \nSo now we\'re going to get into the demo which I thought I had left up here - um okay - and has it crashed? No it hasn\'t! Okay, awesome! It is very buggy. This demo crashes a lot. It\'s, I mean computer vision, is computationally expensive and this is my first stab at it so I don\'t really know how to optimize it yet. So this is an example; this is a built-in function that converts RGB color that\'s coming through my camera into HSV which is hue saturation value. It\'s just a different way of looking at colors which, you know, I have never used this for stuff, but I imagine if you know things were roughly the same shade but very different in in hue or saturation or, you know, value, the third one, um, this might be a way to better detect objects. There\'s also a built in grayscale which just changes it the three channels, it compresses it to one and makes it gray scale. We have contours which it basically looks for contours in the image. And I believe this is just looking for differences in a - great enough difference in shade between nearby pixels. Branching off of contours we have shapes which is it looks for shapes. And you can actually see it breaks sometimes and just shows straight up contours, but for some reason this reminds me of an 80s music video, probably Aha specifically because of the jerky motion. Um and then we have lines which, I like this one, I think it\'s fun but it\'s basically looking for the place where the color changes again and this it\'s taking individual dots and basically making them width one and a line of length 1/50, I think I said 1/50 of the viewscreen for the video. So this might be, you know, as you can see it has a reasonably good edge detection on my shirt, although it also probably thinks all my stripes are edges as well on my shirt. Um so, that\'s just another thing. Now threshold, um, is basically there\'s a value below which everything turns to black a value above which everything turns to white, but this basically my shirt, right now, is a solid black thing. So in terms of if you have a situation where the thing you have is like always going to be darker than than most of its surroundings, or lighter than most of its surroundings, it can help for, you know, object tracking.\nAnd now the big one, um, face detection which I hope works okay. So it\'s a bit screwy it\'s not perfect because these Haar cascades in general are not perfect. So the red circles are supposed to be eyes, the blue circle is face so it\'s pretty good on the face. If I turn to the side it should go away. There is a profile one, um, but I had to build the build this by adding in different Cascades and the more cascades I added in the more likely it was to crash. So I originally had like one for my nose, one for a mouth, one for my profile and it crashed all the time. But, I mean, it\'s easy to see it\'s not perfect occasionally it thinks my mouth is an eye which is pretty typical. Like it\'s a, you know, cause for the eye it looks for like the eyebrow and the darkness underneath. And when I looked for a mouth, it thought every dark thing in the room was a mouth. And right now I have it limited so it will only look for one face and only look for two eyes to hopefully keep down on the computation. That it\'s doing so I\'ll see if i can look at it and make it let\'s see my eyes, but oh it\'s, I can\'t see the screen when I\'m looking at the camera. Um so, I mean, that about wraps it up I just wanted to give a brief introduction and some brief examples and hopefully pique your interest about the field of computer vision.\nThank you.'
  },
  {
    id: 1,
    name:'Motions',
    videoUrl:'https://www.youtube.com/embed/CMZ9WmhWtSc',
    description:'This presentation for Motions was devlivered on Hiring Day which is at the conclusion of the Grace Hopper Program. The resentation was given to gathered recruiters and representatives from external companies participating in Hiring Day. Hiring Day is a joint event with the simultaneous Fullstack Academy cohort.',
    transcript: 'Rita:\nHello everyone and thank you for joining us today. My name is Rita and I\'m here with my teammates Scout, Khristian, and Ally. Motions is a motion capture application that allows users to record themselves performing an action then view that action in 360 degrees. We used the Microsoft Kinect SDK to collect user skeletal data, then fed that data into Unity where we mapped 3D models onto the data as it moved.\nScout:\nThe first step of this process was getting the skeletal data from Kinect into Unity. To produce the skeletal data of a person in the Kinect’s field of view, we used Microsoft’s skeletal tracking. This was - this uses the infrared camera from the Kinect to construct a depth map. It then uses machine learning to compare this depth map to over 100,000 depth maps of known skeletons. We also created character bodies in Blender to give our skeleton a more lifelike look and to give users options for how they wish their captured motions to look. And here we have our three models. One issue we encountered with this task was that the Kinect data does not contain any orientation information it only contains joints and then, by inference, the connections between the joints, or bones. Making the bones within our models point in the correct direction proved to be a difficult task as you can see in this and the example that\'s coming up right after it, there. To solve this issue we calculated the unit directional vector between two successive joints, for example a wrist and an elbow, and then took the appropriate bone and aligned its main axis along that vector. As you can see this resulted in much more human and physically reasonable movement.\nKhristian:\nWhile the Kinect sensor was sufficient to capture the motion data, our web application would not have access to the Kinect sensor. We needed a way to capture and preserve this data to play it back and view it in 360 degrees in our web application. To solve this issue we implemented xml serialization and deserialization to easily store the skeletal data in a file that we can read and write. Our xml file stores each frame of captured motion. Each frame contains information on every body tracked with all joint positions. Since xml was designed to store and transport data it was a nice solution to implement. In the future iterations of motions we would like to switch our file stream from binary - from xml to binary in order to optimize memory storage.\nAlly:\nOverall this project allowed us to explore set of completely new technologies that we had never encountered before. It gave us a taste of Unity which I think we all really liked and we\re interested in exploring further. In the future we plan to implement virtual reality capabilities so that user can actually walk around the 3d model of themselves. There are many possible use cases for a fully realized version of our application including sports analysis, for example perfecting a yoga posture or improving a softball swing, physical therapy or even immersive gaming in which a user could move and interact with virtual objects.Thank you.\n[Applause]',
  },
  {
    id: 2,
    name: 'Degender Your Internet',
    videoUrl: 'https://www.youtube.com/embed/-bApRMy220w',
    description: 'This Degender Your Internet presentation was given upon the conclusion of a weekend long hackathon as part of the Grace Hopper Program. The version of Degender Your Internet was a minimum viable product used as a base for future work.',
    transcript: 'Hi, my name is Scott O\'Keefe and my Stackathon project was Degender Your Internet. It\'s an extension for Google Chrome which seeks to remove gendered language from your internet experience. So the idea behind this was, um, it\'s kind of like there\'s multiple use cases either, you know, you\'re of a minority or oppressed gender identity and you\'re tired of reading people reading things with bias against your gender identity specifically for women or I have a lot of friends who are non-binary and use they/them pronouns and they never see that in articles on the internet. It\'s always he/she so this will take pronouns and make them gender neutral. And this, right now, it\'s biased towards the they/them pronouns rather than some other options like ze, but hopefully in future iterations I\'ll also be able to implement that. And the unofficial tagline is “Warning not suitable for people who are offended by the singular they” because there\'s going to be a lot of that happening.\nSo um, if I can find it. Okay, um, so this is just my sample page, um, which, it has a lot of somewhat garbage text. Um, it has, like, later stanzas of Jack and Jill which apparently exist, but I figured they\'d have pronouns in them. So to use the app once its installed you click here and you click “degender this page” and it pops up. It puts a bar at the top and it changed the pronouns, some adjectives, and some nouns. Um, and right now you can highlight the altered pronouns and un-highlight them, highlight the altered adjectives, and highlight the altered nouns. So the analyze page currently does nothing. Um, which is a bit unfortunate, but the the app does save things when it changes them. So, in adjectives, just have three. Um. my adjectives list is only, like 25, long right now so adjectives, in terms of in the real world, like, not on my page, they don\'t show up very often. But you have nouns which I just had girl in there and pronouns so keeps track of where they occurred and how many. One issue right now is it counts capitalized ones separately than lowercase ones, but that\'s due to my implementation.\nSo the idea, what the analyzed page would be, to send this to the back end — back end, kind of, — of my Chrome Extension. Which is actually a bit more complicated because Chrome Extensions, you don\'t really have a server and the way they use it to keep things safe is you kind of have like your content silo and your rest—  rest of stuff silo. So, you have to send it in a message. There was— there\'s a way to maybe use Redux and hold your store in the page that shared, but I didn\'t understand what was going on when I was reading that because I started that on Thursday. So, if we look at the code a bit—  not that code—  so most of the action happens in my content script and right now I have objects. So. I figured that, so, every word I want to replace is in the object. So it\'s a lookup of n1 because we\'re already having to go over every word in the—  UM an O of N 1, n equals 1, not n equals—  an 0 of 1, that\'s what I meant. Because it just has to look it up in the object, I mean obviously I\'m not covering everything right here and I think the greatest  potential for expansion is in the nouns and adjectives. And ideally eventually this would be in a database probably with firebase that it could check against. And yeah there\'s still a lot of issues. It took me a while to figure out how to get the words when there was punctuation after and right now like right here that doesn\'t get changed because there\'s a break tag right after it and so when it splits it up into words.  So I also didn\'t use any of the natural language libraries which was probably a mistake and in future work I will change that because I think there\'s a lot that can be—  that I could do better. So, for the future, you know, like filling it out more, letting the user have their own settings where they can choose what pronoun to change male or female pronouns to, analyzing the page where it\'ll hopefully get into bias. Like there were this many negative words associated with women, mostly I only have adjectives associated with women right now because there just aren\'t that many that are usually associated with men. I mean there\'s cocky. I have— like cocky is basically the only one I have. Um, but also dealing with stuff like gendered job titles like you know service woman could be you know service member stuff like that. So that\'d be more complicated.\nThe biggest challenges was just basically figuring out how extensions work just because some of the stuff was kind of out of date in the tutorials. Like, it was like, oh go here for the repo and then it was like it had a redirect to another repo which was all about OAuth. And also in the future whoops we\'re going to have settings, where they can do their settings and feedback so they can suggest if they think I\'ve missed a word or something. And then just to show one more feature if I change the language page—  I think “fr” is French—  but we\'ll find out. Then it should, fingers crossed, when I go to degender it a pop-up comes up that says the page isn’t  in English but then also an invitation that if somebody is really interested in this app and they want to help me make it in whatever language the page is in to contact me. So I think that\'s about it also right now none of the pages are really talking to each other the front end for the extension for some reason even though places were like “oh react is perfect for extensions” again react maybe not so much. For some reason I couldn\'t make any of my on clicks work through React so right now that\'s all through the content script which is really less than ideal, but if i get all working through React and then have a store, then I— and this is definitely something I want to keep working on, like, I almost feel like I need help from somebody with a PhD in gender studies because it\'s a lot of complicated language to work out, but I\'m really excited to keep working on it and that is Degender Your Internet.'
  }
]